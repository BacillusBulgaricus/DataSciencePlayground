








# Import the necessary Python packages.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
from sklearn.model_selection import (train_test_split,
                                     KFold, 
                                     cross_val_score,
                                     GridSearchCV)
from sklearn.preprocessing import StandardScaler                             
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (BaggingClassifier, 
                              RandomForestClassifier, 
                              AdaBoostClassifier, 
                              GradientBoostingClassifier)
from sklearn import metrics
from yellowbrick.model_selection import FeatureImportances

import warnings

# Suppress all FutureWarnings.
warnings.simplefilter(action='ignore', category=FutureWarning)


# Read in the dataset into a DataFrame: orig_df.
orig_df = pd.read_csv('datasets/recipe_site_traffic_2212.csv')
orig_df.head()


# Print a concise summary of the DataFrame.
orig_df.info()


# Generate descriptive statistics of all columns in the DataFrame.
orig_df.describe(include='all')





# Create a working copy of the original DataFrame: tb_df.
tb_df = orig_df
tb_df.shape


# Check for duplicated values.
print(tb_df.duplicated().sum())


# Check for missing values.
missing_values = tb_df.isna().sum().to_frame(name='missing_count')

# Summarize missing data per column as a number and in percentages.
missing_values['missing_%'] = round(tb_df.isna().mean()*100, 2)
missing_values


# Visualize missing values in a matrix.
msno.matrix(tb_df)
plt.title("Missing Data",
          fontsize=14)
plt.show()








# Inspect the recipe column.
print(tb_df['recipe'].describe(),'\n')
print(f"The 'recipe' column is of data type {tb_df['recipe'].dtype}.") 


# Set the recipe column as an index for the tb_df DataFrame.
tb_df.set_index('recipe',
                inplace=True)
tb_df.head()





# Inspect the high_traffic_column.
print(f"'high_traffic' data type: '{tb_df['high_traffic'].dtype}'.", '\n')
print(f"'high_traffic' missing values: {tb_df['high_traffic'].isna().sum()}")


# Replace missing values with `Low` label for low web traffic.
tb_df['high_traffic'].fillna("Low", inplace=True)
print(tb_df['high_traffic'].describe(), '\n')
print(tb_df['high_traffic'].value_counts())





# Inspect the calories column.
print(tb_df['calories'].describe(), '\n')
print(f"The 'calories' column is of data type {tb_df['calories'].dtype}.")


# Create a dictionary with calories median values per category.
calories_dict = tb_df.groupby('category')['calories'].median().to_dict()
calories_dict


# Impute the missing data with the calories_dict.
tb_df['calories'] = tb_df['calories'].fillna(tb_df['category'].map(calories_dict))
print(tb_df['calories'].describe())





# Inspect the carbohydrate column.
print(tb_df['carbohydrate'].describe(), '\n')
print(f"The 'carbohydrate' column is of data type {tb_df['carbohydrate'].dtype}.")


# Create a dictionary with carbohydrate median values per category.
carbohydrate_dict = tb_df.groupby('category')['carbohydrate'].median().to_dict()
carbohydrate_dict


# Impute the missing data with the carbohydrate_dict.
tb_df['carbohydrate'] = tb_df['carbohydrate'].fillna(tb_df['category'].map(carbohydrate_dict))
print(tb_df['carbohydrate'].describe())





# Inspect the sugar column.
print(tb_df['sugar'].describe(), '\n')
print(f"The 'sugar' column is of data type {tb_df['sugar'].dtype}.")


# Create a dictionary with sugar median values per category.
sugar_dict = tb_df.groupby('category')['sugar'].median().to_dict()
sugar_dict


# Impute the missing data with the sugar_dict.
tb_df['sugar'] = tb_df['sugar'].fillna(tb_df['category'].map(sugar_dict))
print(tb_df['sugar'].describe())





# Inspect the protein column.
print(tb_df['protein'].describe(), '\n')
print(f"The 'protein' column is of data type {tb_df['protein'].dtype}.")


# Create a dictionary with protein median values per category.
protein_dict = tb_df.groupby('category')['protein'].median().to_dict()
protein_dict


# Impute the missing data with the protein_dict.
tb_df['protein'] = tb_df['protein'].fillna(tb_df['category'].map(protein_dict))
print(tb_df['protein'].describe())





# Inspect the category column.
print(tb_df['category'].describe(), '\n')
tb_df['category'].unique()


# Assign the 'Chicken Breast' category to 'Chicken'.
tb_df[tb_df['category'] == 'Chicken Breast']['category'].count()
tb_df['category'] = tb_df['category'].str.replace('Chicken Breast', 'Chicken')

# Convert category column to pandas category data type.
tb_df['category'] = tb_df['category'].astype('category')

# Check the category column after data modification.
print(tb_df['category'].describe(), '\n')
tb_df['category'].dtype





# Inspect the servings column.
display(tb_df['servings'].describe())
print(f"The 'servings' column is of data type {tb_df['servings'].dtype}.")
tb_df['servings'].unique()


# Remove the `as a snack` part from the servings column.
tb_df['servings'] = tb_df['servings'].str.rstrip(' as a snack')

# Change the data type of the servings column to integer.
tb_df['servings'] = tb_df['servings'].astype('int')

# Check the servings columns after data modification.
display(tb_df['servings'].describe())
print(f"The 'servings' column is of data type {tb_df['servings'].dtype}.")


# Dataset structure after data validation and cleaning.
print(tb_df.dtypes, '\n')
tb_df.describe(include='all')








# Calculate the counts of recipes by web traffic.
web_traffic = tb_df['high_traffic'].value_counts().to_frame()
web_traffic['percentage'] = round(tb_df['high_traffic'] \
                                  .value_counts(normalize=True)*100, 2)
web_traffic


# Visualize the high_traffic column.
sns.set(rc={"figure.dpi":150})
sns.set_style('darkgrid')
ax = sns.barplot(data=web_traffic,
                 x=web_traffic.index,
                 y='count')
for i in ax.containers:
    ax.bar_label(i, padding=1.04)
ax.bar_label(ax.containers[0],
             labels=['60.61%', '39.39%'],
             label_type='center',
             color='white',
             fontsize=10)
plt.title("Recipes by generated web traffic",
          y=1.04,
          fontsize=13)
plt.xlabel("Web traffic")
plt.ylabel("Number of recipes")
plt.yticks(range(0, 750, 100))
plt.show()





# Explore the recipes distribution by category.
sorted_cat = tb_df['category'].value_counts().reset_index()
sorted_cat['percentage'] = sorted_cat['count'].apply(
    lambda c: round(c / sorted_cat['count'].sum() * 100, 2))
                                
display(sorted_cat)

ax = sns.barplot(data=sorted_cat,
                 x='count',
                 y='category', 
                 order=sorted_cat['category'])

ax.bar_label(ax.containers[0])
plt.title("Number of recipes per category",
          y=1.03,
          fontsize=13)
plt.xlabel("Number of recipes")
plt.ylabel("Category")
plt.xticks(range(0, 220, 20))
plt.show()





# Extract data for web traffic by category.
cat_traffic = tb_df.groupby(['category', 'high_traffic'],
                            observed=False)['high_traffic'] \
                   .agg(count = 'count')

# Create a pivot table for web traffic by category.
traff_cat = pd.pivot_table(data=cat_traffic,
                           index='category',
                           columns='high_traffic',
                           values='count').sort_values('High', 
                                                       ascending=False)
traff_cat





# Plot web traffic by category.
sns.catplot(kind='count',
            data=tb_df,
            x='category',
            hue='high_traffic',
            order=traff_cat.index,
            legend=False,
            aspect=10/5
            )
plt.title("Web traffic by category",
          y=1.03,
          fontsize=12)
plt.xlabel("")
plt.xticks(fontsize=9)
plt.ylabel("Number of recipes")
plt.legend(title="Web traffic",
           loc='best',
           bbox_to_anchor=(0.2, 0.5, 0.7, 0.5))
plt.show()





# Merge the tables sorted_cat and traff_cat.
recip_traff = sorted_cat[['category', 'count']].merge(traff_cat,
                               on='category')
recip_traff.sort_values(by=['count', 'Low', 'High'],
                        ascending=False)





# Create frequency distribution histograms for all numerical variables.
fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(nrows=1,
                                              ncols=5,
                                              sharey=True,
                                              figsize=(20, 5))

fig.suptitle("Frequency distribution",
             fontsize=20,
             y=1.05)

# calories column
ax1.set_title("Calories", fontsize=15)
ax1.hist(tb_df['calories'], bins=20)

# carbohydrate column
ax2.set_title("Carbohydrate", fontsize=15)
ax2.hist(tb_df['carbohydrate'], bins=20)

# sugar column
ax3.set_title("Sugar", fontsize=15)
ax3.hist(tb_df['sugar'], bins=20)

# protein column
ax4.set_title("Protein", fontsize=15)
ax4.hist(tb_df['protein'], bins=20)

# servings column
ax5.set_title("Servings", fontsize=15)
ax5.hist(tb_df['servings'], bins=6)

plt.show()





# Plot the numerical variables frequency distribution by category.
fig, ax = plt.subplots(nrows=1,
                       ncols=5,
                       sharey=True,
                       figsize=(20, 5))

fig.suptitle("Frequency distribution per recipe category",
             fontsize=20,
             y=1.05)

# Plot calories frequency by recipe category.
sns.boxplot(data=tb_df,
            x='calories',
            y='category',
            order=traff_cat.index,
            ax=ax[0],
            )
ax[0].set_title('Calories\nper category', fontsize=15)
ax[0].set_ylabel("Categories")

# Plot carbohydrate frequency by recipe category.
sns.boxplot(data=tb_df,
            x='carbohydrate',
            y='category',
            order=traff_cat.index,
            ax=ax[1],
            )
ax[1].set_title('Carbohydrate\nper category', fontsize=15)
ax[1].set(ylabel='')

# Plot sugar frequency by recipe category.
sns.boxplot(data=tb_df,
            x='sugar',
            y='category',
            order=traff_cat.index,
            ax=ax[2],
            )
ax[2].set_title('Sugar\nper category', fontsize=15)
ax[2].set(ylabel='')

# Plot protein frequency by recipe category.
sns.boxplot(data=tb_df,
            x='protein',
            y='category',
            order=traff_cat.index,
            ax=ax[3],
            )
ax[3].set_title('Protein\nper category', fontsize=15)
ax[3].set(ylabel='')

# Plot servings frequency by recipe category.
sns.boxplot(data=tb_df,
            x='servings',
            y='category',
            order=traff_cat.index,
            ax=ax[4],
            )
ax[4].set_title('Servings\nper category', fontsize=15)
ax[4].set(ylabel='')

plt.show()





# Create a PairGrid plot of the pairwise relationships of the numeric variables.
g = sns.PairGrid(tb_df, corner=True, 
                 diag_sharey=False, 
                 hue='high_traffic')
g.map_lower(sns.scatterplot, alpha=0.3)
g.map_diag(sns.kdeplot, cut=0)
g.fig.suptitle("Pairwise relationship of numerical variables",
               fontsize=18)
g.add_legend(title="Web traffic",
             bbox_to_anchor=(.0, 0.6, 0.7, 0.6))
plt.show()





# Plot a heatmap of the correlation between numeric variables.
sns.heatmap(tb_df.corr(numeric_only=True,
                       method='pearson'),
            annot=True,
            fmt='.2f',            
           )
plt.title("Correlation of numerical variables",
          y=1.03)
plt.show()








# Create a new feature recipe_popularity based on high_traffic.
tb_df['recipe_popularity'] = tb_df['high_traffic'].apply(
    lambda cat: 1 if cat == 'High' else 0
)

# Check the correct encoding of the recipe_popularity.
display(tb_df[['high_traffic', 'recipe_popularity']].head())
tb_df.value_counts(subset=['high_traffic', 'recipe_popularity']).to_frame()


# Drop high_traffic column and create DataFrame for ML model: tb_ml.
tb_ml = tb_df.drop('high_traffic', axis=1)
tb_ml.head()





# Encode categorical data to numerical for the ML model.
tb_ml = pd.get_dummies(tb_ml, 
                       drop_first=True,
                       dtype='int')
tb_ml.head()





# Create a DataFrame X containing the input features
# and a Series y with labels for the target variable.
X = tb_ml.drop('recipe_popularity',
               axis=1).values
y = tb_ml['recipe_popularity'].values

# Check the shape of X and y.
X.shape, y.shape





# Split data into training and test sets.
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2,
                                                    stratify=y,
                                                    random_state=15)





# Standardize the data for ML model training.
# Instantiate the scaler.
std_scaler = StandardScaler()
X_train_scaled = std_scaler.fit_transform(X_train)
X_test_scaled = std_scaler.transform(X_test)

# Check the results of the standardization.
print(np.mean(X_train), np.std(X_train))
print(np.mean(X_train_scaled), np.std(X_train_scaled))





# Create a baseline ML model with a DummyClassifier.
baseline_ml = DummyClassifier(strategy='stratified',
                              random_state=15)
baseline_ml.fit(X_train_scaled, y_train)
y_pred_bs_ml = baseline_ml.predict(X_test)

# Calculate accuracy.
bs_ml_accuracy = round(baseline_ml.score(X_test_scaled, y_test) * 100, 2)
print(bs_ml_accuracy)

# Calculate precision.
bs_ml_precision = round(metrics.precision_score(y_test, y_pred_bs_ml) * 100, 2)
print(bs_ml_precision)








# Standardize the input features.
x_scaler = StandardScaler()
X_scaled = x_scaler.fit_transform(X)

# Test various classifier algorithms.
for model in [KNeighborsClassifier,
              GaussianNB,
              LogisticRegression,
              DecisionTreeClassifier,
              BaggingClassifier,
              RandomForestClassifier,
              AdaBoostClassifier,
              GradientBoostingClassifier]:
    classifier = model()
    kfold = KFold(n_splits=10, 
                  random_state=15,
                  shuffle=True
                 )
    cv_score = cross_val_score(classifier, X_scaled, y,
                               scoring='roc_auc',
                               cv=kfold)
    print(f"{model.__name__:27} AUC: {cv_score.mean():.3f} "
          f"STD: {cv_score.std():.2f}")





# Develop a comparison ML model with a LogistidRegression classifier.
# Instantiate the classifier.
lg = LogisticRegression(random_state=15)

# Fit the ML model to the training data.
lg.fit(X_train_scaled, y_train)

# Predict labels on the test set.
y_pred = lg.predict(X_test_scaled)

# Calculate ML model performance metrics.
lg_accuracy = round(lg.score(X_test_scaled, y_test) * 100, 2)
lg_precision = round(metrics.precision_score(y_test, y_pred) * 100, 2)
print(f"LogisticRegression ML model accuracy: {lg_accuracy}")
print(f"LogisticRegression ML model precision: {lg_precision}")





# Visualize feature importance.
labels = tb_ml.drop('recipe_popularity', axis=1).columns

fig, ax = plt.subplots(figsize=(6, 4))
fi_viz = FeatureImportances(lg, labels=labels)
fi_viz.fit(X_train_scaled, y_train)
fi_viz.poof()
fig.show()





# New model with category only.
X2 = tb_ml.drop(['recipe_popularity', 'calories', 'carbohydrate', 'sugar', \
                'protein', 'servings'], axis=1).values
y2 = tb_ml['recipe_popularity'].values

# Check the shape of X2 and y2.
X2.shape, y2.shape


# Split data into training and test sets.
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2,
                                                        test_size=0.2,
                                                        stratify=y,
                                                        random_state=15)

# Instantiate the classifier.
log_reg2 = LogisticRegression(random_state=15)
log_reg2.fit(X_train2, y_train2)
y_pred2 = log_reg2.predict(X_test2)

# Calculate performance metrics.
accurary2 = round(log_reg2.score(X_test2, y_test2) * 100, 2)
print(accurary2)
precision2 = round(metrics.precision_score(y_test2, y_pred2) * 100, 2)
print(precision2)





# Perform exhaustive search over specified parameter values for the estimator.
lg_hp = LogisticRegression(random_state=15)

# Define dictionary with hyperparameters to optimize.
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'solver': ['liblinear', 'lbfgs', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],
    'penalty': ['l2'],
    'random_state': [15]
}

lg_cv = GridSearchCV(lg_hp, param_grid, cv=5, n_jobs=-1) \
            .fit(X_train2, y_train2)

print(lg_cv.best_params_)





# Build a new ML model with the optimized hyperparamerers.
lg_opt = LogisticRegression(
    **{'C': 1, 
       'penalty': 'l2', 
       'random_state': 15, 
       'solver': 'liblinear'}
)

lg_opt.fit(X_train2, y_train2)

y_pred_opt = lg_opt.predict(X_test2)

# Calculate ML model performance metrics.
lg_opt_accuracy = round(lg_opt.score(X_test2, y_test2) * 100, 2)
lg_opt_precision = round(metrics.precision_score(y_test2, y_pred_opt) * 100, 2)
print(f"LogisticRegression ML model accuracy: {lg_opt_accuracy}")
print(f"LogisticRegression ML model precision: {lg_opt_precision}")








# Create a DataFrame with the performance metrics of the ML models.
data = {'ML model': ['DummyClassifier', 
                     'LogisticRegression (14 features)', 
                     'LogisticRegression (9 features)'],
        'Accuracy' : [bs_ml_accuracy, lg_accuracy, lg_opt_accuracy],
        'Precision' : [bs_ml_precision, lg_precision, lg_opt_precision]}

perf_metrics_df = pd.DataFrame(data)
perf_metrics_df


# Visualize and compare performance statistics of ML models.
ax = perf_metrics_df.plot.barh(label=["One", 2, 3],
                               tick_label=5,)
ax.bar_label(ax.containers[0], fontsize=10)
ax.bar_label(ax.containers[1], fontsize=11)
plt.xticks(range(0, 120, 20),
           fontsize=8)
plt.yticks([0, 1, 2], 
           ["DummyClassifier", 
            "LogisticRegression\n (14 features)", 
            "LogisticRegression\n (9 features)"])
#ax.yticks = perf_metrics_df['ML model']
plt.title("Performance metrics: accuracy & precision (in %)")
plt.legend(reverse=True)
plt.show()






